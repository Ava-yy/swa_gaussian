{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "# import time\n",
    "# import tabulate\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import torchvision\n",
    "import os\n",
    "os.sys.path.append(\"/home/izmailovpavel/Documents/Projects/private_swa_uncertainties/\")\n",
    "from swag import data, models, utils, losses\n",
    "from swag.posteriors import SWAG\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine\n",
    "# raw_data = np.loadtxt(\"/home/izmailovpavel/datasets/UCI/Regression/winequality-red.csv\", delimiter=\";\", skiprows=1)\n",
    "\n",
    "# concrete\n",
    "# raw_data = pd.ExcelFile(\"/home/izmailovpavel/datasets/UCI/Regression/Concrete_Data.xls\")\n",
    "\n",
    "# CCPP\n",
    "x = np.load(\"/home/izmailovpavel/datasets/UCI/Regression/CCPP/x.npy\")\n",
    "y = np.load(\"/home/izmailovpavel/datasets/UCI/Regression/CCPP/y.npy\")\n",
    "raw_data = np.hstack([x, y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a random $0.9$ to $0.1$ train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.9 * raw_data.shape[0])\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(raw_data)\n",
    "y = raw_data[:, -1]\n",
    "x = raw_data[:, :-1]\n",
    "x_tr, y_tr = x[:n_train], y[:n_train]\n",
    "x_te, y_te = x[n_train:], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "# print(x_tr)\n",
    "x_tr = scaler_x.fit_transform(x_tr)\n",
    "# print(x_tr)\n",
    "# print(x_te)\n",
    "x_te = scaler_x.transform(x_te)\n",
    "# print(x_te)\n",
    "y_tr = scaler_y.fit_transform(y_tr[:, None])[:, 0]\n",
    "y_te = scaler_y.transform(y_te[:, None])[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pytorch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromNumpy(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, x, y, cuda=False):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        if cuda:\n",
    "            self.x.cuda()\n",
    "            self.y.cuda()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DatasetFromNumpy(x_tr, y_tr)\n",
    "testset = DatasetFromNumpy(x_te, y_te)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in testloader:\n",
    "# for x, y in trainloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNetBase(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, n_hidden=50, p=0.):\n",
    "        \"\"\"Using the architecture from Concrete Dropout\"\"\"\n",
    "        super(RegressionNetBase, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p),\n",
    "            torch.nn.Linear(num_inputs, n_hidden),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(p),\n",
    "            torch.nn.Linear(n_hidden, n_hidden),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(p),\n",
    "            torch.nn.Linear(n_hidden, 2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class RegressionNet:\n",
    "    base = RegressionNetBase\n",
    "    args = list()\n",
    "    kwargs = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL_(mus, sigmas2, target):\n",
    "    loss = ((mus - target)**2 / (sigmas2) + torch.log(sigmas2) + torch.log(torch.Tensor([3.1415 * 2]))) / 2\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def NLL(model, input, target):\n",
    "    # standard cross-entropy loss function\n",
    "\n",
    "    output = model(input)\n",
    "    n = len(input)\n",
    "    mus = output[:, 0]\n",
    "    log_sigmas2 = output[:, 1]\n",
    "    sigmas2 = torch.exp(log_sigmas2)\n",
    "    \n",
    "    \n",
    "    loss = ((mus - target)**2 / (sigmas2) + log_sigmas2 + torch.log(torch.Tensor([3.1415 * 2]))) / 2\n",
    "#     print(loss.shape)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionNet.base(num_inputs=x_tr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    #lr=1e-5, # wine\n",
    "    lr=5e-4, # CCPP\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1787230660034629\n",
      "0.44345936821169063\n",
      "0.11460787731422571\n",
      "0.06440493340913656\n",
      "0.045386617585987235\n",
      "0.036021674133429694\n",
      "0.027325977285887607\n",
      "0.021586240923040602\n",
      "0.015675506778965564\n",
      "0.01069341611189372\n",
      "0.009793044423208379\n",
      "0.009363220721888162\n",
      "0.008303142710914526\n",
      "0.00421882224650025\n",
      "0.0031473721697582156\n",
      "0.0018647329003785133\n",
      "-0.001114381117945888\n",
      "0.0004759941169185902\n",
      "0.0014445708543079515\n",
      "0.00012575319522889895\n",
      "-0.0021551595915348157\n",
      "-0.0031603812693774304\n",
      "-0.002347842026720367\n",
      "-0.002749559788902716\n",
      "-0.0013063120905661635\n",
      "-0.0003618090503820212\n",
      "-0.0014150244075791538\n",
      "-0.00020036680013678544\n",
      "-0.00190393761580808\n",
      "-0.00358301634406977\n",
      "-0.0027108396917335832\n",
      "-0.0007857487545294735\n",
      "-0.0021098443349054806\n",
      "-0.0035701138420036315\n",
      "-0.004088593386230451\n",
      "-0.0029294706933825927\n",
      "-0.0023383474389084427\n",
      "-0.0014134386651483567\n",
      "-0.00245225687376396\n",
      "-0.003443561548860546\n",
      "0.0009649213411683143\n",
      "0.0002407093103332617\n",
      "-0.003441280250474006\n",
      "-0.0026485499307508263\n",
      "-0.0024715639713984196\n",
      "-0.0018256329629025578\n",
      "0.0004103783219135565\n",
      "-0.002570837362346133\n",
      "-0.0030812153731360416\n",
      "-0.003397271128817173\n",
      "-0.002888626364822131\n",
      "0.001710674583032812\n",
      "-0.0018513821902112835\n",
      "0.0023823484264451746\n",
      "-0.0006281195333673763\n",
      "0.00027254881751951254\n",
      "-0.001674273290668488\n",
      "-0.0041041273873326616\n",
      "0.0006283759705669553\n",
      "-0.0014096475352865523\n",
      "-0.0015867518369446314\n",
      "0.0018041878973284523\n",
      "0.0021146914952308816\n",
      "0.0011748938786971031\n",
      "0.0014076787217432247\n",
      "0.0013059804302767558\n",
      "0.0003549129016556373\n",
      "-0.0004894098533236763\n",
      "-0.0007897160437696556\n",
      "0.0005474498657908344\n",
      "0.0008641781257068552\n",
      "0.00013666926270895698\n",
      "0.022079307331141523\n",
      "-0.0025737481590846233\n",
      "0.0007098005519343397\n",
      "0.0005970538283748336\n",
      "-0.002044112248199768\n",
      "0.0018773976881708116\n",
      "-0.0024067004540146274\n",
      "0.0007891744762856348\n",
      "0.0026758750661603665\n",
      "-0.00021375495442501304\n",
      "0.0022050242586814767\n",
      "0.0032137244681962594\n",
      "0.0009408541540279439\n",
      "-4.841712688599545e-05\n",
      "-0.00019204282564037618\n",
      "-0.002398405594840142\n",
      "0.001518914843115044\n",
      "-0.0008593229082677024\n",
      "0.0034331166776073005\n",
      "0.0012602130996545507\n",
      "0.0014379111514137453\n",
      "0.004073547447908632\n",
      "0.0014565423478815505\n",
      "0.0005652297717770998\n",
      "0.00335268800183326\n",
      "0.0020327971567469126\n",
      "0.001358788179722747\n",
      "0.0023623281505143596\n",
      "0.0027863561134556557\n",
      "0.0015441350991019056\n",
      "0.0011817878677847493\n",
      "0.0012180430020982554\n",
      "0.000584420671156787\n",
      "0.001844007951459352\n",
      "0.0021442334184855927\n",
      "-0.00042085612765755087\n",
      "0.004112920252624226\n",
      "-0.001029430781627917\n",
      "0.0001443485988860152\n",
      "-0.0012116778629825806\n",
      "0.0017895922298395957\n",
      "0.0015211451806031984\n",
      "0.00273994249839523\n",
      "-0.0002077944847560347\n",
      "0.0014561954766750724\n",
      "0.0009938178716145\n",
      "0.0008199969874401843\n",
      "0.00039906764664504325\n",
      "0.0009708902088555568\n",
      "0.0024739300918460044\n",
      "0.0019180557980740642\n",
      "4.4905669375477096e-05\n",
      "0.0015827490399903004\n",
      "0.001819923040745303\n",
      "-0.00032769310745734976\n",
      "0.0022197485111992116\n",
      "0.0011032574746600704\n",
      "0.0017501512148033907\n",
      "0.0030275721670982936\n",
      "0.0018872091225001777\n",
      "0.0005211427923750066\n",
      "0.0029664228234109998\n",
      "0.0008100455970994521\n",
      "0.0005373263679900772\n",
      "0.0023423405334953924\n",
      "0.002110955230229796\n",
      "0.003115011866036476\n",
      "0.0032064392929112588\n",
      "0.00039241712048499237\n",
      "0.000580290022031755\n",
      "0.0003704144597233498\n",
      "0.0033212901788394135\n",
      "0.0007863760672854901\n",
      "0.005274790856477179\n",
      "0.003012007640333157\n",
      "-0.00012334257506172412\n",
      "0.002529291288120145\n",
      "0.0012751728765239923\n",
      "0.0019424218096504393\n",
      "0.004018046070011827\n",
      "0.001452963784961747\n",
      "-0.0004141857650080371\n",
      "-0.0004992414010192664\n",
      "0.0026643930405167225\n",
      "0.0012265951870736754\n",
      "0.002941993972923182\n",
      "0.0030572343517742484\n",
      "0.005254302368099273\n",
      "0.004671607220606194\n",
      "0.002970984935649747\n",
      "0.001702353149745927\n",
      "0.00041322182894666746\n",
      "-0.0004830894438650893\n",
      "0.00171764591786002\n",
      "0.00022878559357529386\n",
      "0.0013187967613728783\n",
      "0.0027400862047888513\n",
      "0.0040208286433917086\n",
      "0.0029156108864535164\n",
      "0.0002548066929896428\n",
      "0.007438708896548743\n",
      "0.001452187709782877\n",
      "0.0004676654839706731\n",
      "0.0038123762215793847\n",
      "0.0005478956408373862\n",
      "0.0008799040750847531\n",
      "0.0026877976892551274\n",
      "0.0015570708177269938\n",
      "0.001030636971011531\n",
      "0.0015743835564759423\n",
      "0.001963860224990836\n",
      "0.004705871034756644\n",
      "0.002695957839232872\n",
      "0.0036725503647871485\n",
      "0.001958366803156071\n",
      "0.0032322046206036507\n",
      "0.002035411183128871\n",
      "0.0015365343665068828\n",
      "0.0004513468245099942\n",
      "0.0013903333617203632\n",
      "0.0011033474492245941\n",
      "0.0026683891474363103\n",
      "0.00044191857550482264\n",
      "8.419317182189371e-05\n",
      "0.0012411335012719213\n",
      "0.0011213415989877456\n",
      "-0.0016131012534696513\n",
      "0.004609222742665619\n",
      "0.0009136274647330618\n",
      "-0.005879497787821183\n",
      "-0.00709445990692128\n",
      "-0.0071007923795542586\n",
      "-0.007635111858130138\n",
      "-0.007815210177028737\n",
      "-0.007575810607987793\n",
      "-0.007918365616379486\n",
      "-0.008237658247426057\n",
      "-0.008074425878168716\n",
      "-0.007649773394600493\n",
      "-0.008072477643490669\n",
      "-0.0057375799237664275\n",
      "-0.007235425341307196\n",
      "-0.007843696991732943\n",
      "-0.007917796457260968\n",
      "-0.0075308775849187845\n",
      "-0.007974227191042144\n",
      "-0.008271386545580198\n",
      "-0.007889139677863298\n",
      "-0.008188071393230183\n",
      "-0.008318146610907901\n",
      "-0.008001001462843925\n",
      "-0.007738015669284555\n",
      "-0.008164020680257105\n",
      "-0.008700127798067597\n",
      "-0.0075319482362058545\n",
      "-0.008006766322542049\n",
      "-0.008120137198352603\n",
      "-0.008151825222801822\n",
      "-0.00825840490521378\n",
      "-0.007767797945794737\n",
      "-0.008032623258371854\n",
      "-0.0080730287078476\n",
      "-0.008081819133162705\n",
      "-0.007964302180134839\n",
      "-0.008625458874048055\n",
      "-0.008285577979919568\n",
      "-0.008011600570892737\n",
      "-0.008562434764498687\n",
      "-0.008013909354921888\n",
      "-0.007950811214583246\n",
      "-0.00814263467941809\n",
      "-0.0078018269788345355\n",
      "-0.008741974083060187\n",
      "-0.007332123470782624\n",
      "-0.008135107388383987\n",
      "-0.008003527557492713\n",
      "-0.008163232938183709\n",
      "-0.007704419460164491\n",
      "-0.008757875143145728\n",
      "-0.008495809319348523\n",
      "-0.007953849422220098\n",
      "-0.007991202257815007\n",
      "-0.008271873523566092\n",
      "-0.00868103489402611\n",
      "-0.008260643825881531\n",
      "-0.00865307119772861\n",
      "-0.00878408797290347\n",
      "-0.007733843319058321\n",
      "-0.007706233460013563\n",
      "-0.008379568095898576\n",
      "-0.008701006182670704\n",
      "-0.007697144561549843\n",
      "-0.008696630095275434\n",
      "-0.008282807615074377\n",
      "-0.007321888428852404\n",
      "-0.00786605569048496\n",
      "-0.007818341224039353\n",
      "-0.008302427607675751\n",
      "-0.008800985138669079\n",
      "-0.00864949460063381\n",
      "-0.00799152038099389\n",
      "-0.008179747718106771\n",
      "-0.008317227490811298\n",
      "-0.00818956284571256\n",
      "-0.00827908821878997\n",
      "-0.008339955125684565\n",
      "-0.008465258325659402\n",
      "-0.007994751338983797\n",
      "-0.008032257209988701\n",
      "-0.008032383306587688\n",
      "-0.008257750496605716\n",
      "-0.00853088431492069\n",
      "-0.008002528986311612\n",
      "-0.008739050152845575\n",
      "-0.008398229462415717\n",
      "-0.008521610841755529\n",
      "-0.009607318536549437\n",
      "-0.00843693070081645\n",
      "-0.008487920870267061\n",
      "-0.008056840671920954\n",
      "-0.007767388246059694\n",
      "-0.008675883385573319\n",
      "-0.008810305828839668\n",
      "-0.008236527269884154\n",
      "-0.008119340274137302\n",
      "-0.008133037767472852\n",
      "-0.00786831984976736\n",
      "-0.008031477461866545\n",
      "-0.008074060049556492\n",
      "-0.008014270160383996\n",
      "-0.008384658240673918\n",
      "-0.008179184169203628\n",
      "-0.007868919932126529\n",
      "-0.008476626134447596\n",
      "-0.008247306082199127\n",
      "-0.008552947189455982\n",
      "-0.00817025897672071\n",
      "-0.00861887452619276\n",
      "-0.008377547686370791\n",
      "-0.008376663291811486\n",
      "-0.008585984034621887\n",
      "-0.008099230514449284\n",
      "-0.008076808977218445\n",
      "-0.007896427160743055\n",
      "-0.008342607428548284\n",
      "-0.008667645864115127\n",
      "-0.00853994778225389\n",
      "-0.008563284592724832\n",
      "-0.00820732807160642\n",
      "-0.008303026204850667\n",
      "-0.008109294895167938\n",
      "-0.008175126028910925\n",
      "-0.008989221424525408\n",
      "-0.008183913402524978\n",
      "-0.008692027135062005\n",
      "-0.008506039697635798\n",
      "-0.008061744951922198\n",
      "-0.008391767233219065\n",
      "-0.008461983285428146\n",
      "-0.008997358219948343\n",
      "-0.00811068854006917\n",
      "-0.00850731047890685\n",
      "-0.008176073307374146\n",
      "-0.007877216898160119\n",
      "-0.008192388871004848\n",
      "-0.008377017251928291\n",
      "-0.008507726372028944\n",
      "-0.008019826602830578\n",
      "-0.008486648552329985\n",
      "-0.008520928489360338\n",
      "-0.00831442967536571\n",
      "-0.008423777256689203\n",
      "-0.008152613329790137\n",
      "-0.007985941306900416\n",
      "-0.008143284756635321\n",
      "-0.008738683330072728\n",
      "-0.00838781938764944\n",
      "-0.00818012379462012\n",
      "-0.008687711311624297\n",
      "-0.007971784321221585\n",
      "-0.00813642055267342\n",
      "-0.008404043225471794\n",
      "-0.008734938026775869\n",
      "-0.008169505082831372\n",
      "-0.00878386776762382\n",
      "-0.008417373130080896\n",
      "-0.008830778289514058\n",
      "-0.008334867534664723\n",
      "-0.008063649178769528\n",
      "-0.008431271900492199\n",
      "-0.00849052675690486\n",
      "-0.007593188527852536\n",
      "-0.00868426091538989\n",
      "-0.008263997916145082\n",
      "-0.008272691126581877\n",
      "-0.008254197843294085\n",
      "-0.007953959152806776\n",
      "-0.008167695997544813\n",
      "-0.008148332842965393\n",
      "-0.007255529508733567\n",
      "-0.008295365531897797\n",
      "-0.008081500423783805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.008698578219202778\n",
      "-0.008417733074196704\n",
      "-0.008021331076740513\n",
      "-0.008062447426336012\n",
      "-0.009096284622449574\n",
      "-0.008707620356642484\n",
      "-0.008447015422026796\n",
      "-0.008282759453227\n",
      "-0.008214518915844108\n",
      "-0.008482378017278016\n",
      "-0.008349036210815869\n",
      "-0.00861370921370294\n",
      "-0.008548933082085954\n",
      "-0.008549894519334553\n",
      "-0.008088370525328875\n",
      "-0.008323580480012284\n",
      "-0.008627702842957882\n",
      "-0.008058444044479522\n",
      "-0.008665855153278943\n",
      "-0.008556968045697064\n",
      "-0.008273856348587707\n",
      "-0.008542881984596342\n",
      "-0.008305036499724267\n",
      "-0.008422386804955745\n",
      "-0.009031294821128842\n",
      "-0.008394008199313891\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):\n",
    "    train_res = utils.train_epoch(trainloader, model, NLL, optimizer, cuda=False, regression=True)\n",
    "    print(train_res['loss'])\n",
    "    if i == 200:\n",
    "        utils.adjust_learning_rate(optimizer, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': None, 'loss': 0.00376473987512018}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval(testloader, model, NLL, cuda=False, regression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RegressionNet.base(num_inputs=11)\n",
    "swag_model = SWAG(RegressionNet.base, no_cov_mat=True, max_num_models=20, num_inputs=x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.adjust_learning_rate(optimizer, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.005988671858265569 tensor(0.0079)\n",
      "-0.0056200391281884575 tensor(0.0106)\n",
      "-0.005599050364169271 tensor(0.0166)\n",
      "-0.004909198422725989 tensor(0.0061)\n",
      "-0.006231211493577626 tensor(0.0088)\n",
      "-0.005106955797826492 tensor(0.0056)\n",
      "-0.005896788065808117 tensor(0.0088)\n",
      "-0.006152766916674613 tensor(0.0129)\n",
      "-0.006276586418879487 tensor(0.0071)\n",
      "-0.006328958900752016 tensor(0.0073)\n",
      "-0.005810057572850006 tensor(0.0093)\n",
      "-0.006152805696725984 tensor(0.0068)\n",
      "-0.005721862116295789 tensor(0.0053)\n",
      "-0.006079263822491915 tensor(0.0098)\n",
      "-0.00650232204523458 tensor(0.0141)\n",
      "-0.006288965755599659 tensor(0.0086)\n",
      "-0.005728356835249893 tensor(0.0056)\n",
      "-0.006234924758772252 tensor(0.0073)\n",
      "-0.0058159057482265455 tensor(0.0071)\n",
      "-0.005510589917519272 tensor(0.0064)\n",
      "-0.006014433299300424 tensor(0.0063)\n",
      "-0.006711146006350484 tensor(0.0127)\n",
      "-0.005517102784014596 tensor(0.0075)\n",
      "-0.00619590014561919 tensor(0.0052)\n",
      "-0.006354206198946052 tensor(0.0056)\n",
      "-0.006243775034460259 tensor(0.0061)\n",
      "-0.0062030316672803025 tensor(0.0117)\n",
      "-0.0053242763893527585 tensor(0.0084)\n",
      "-0.0061598961921729995 tensor(0.0086)\n",
      "-0.007426444681184159 tensor(0.0064)\n",
      "-0.006618758517958508 tensor(0.0053)\n",
      "-0.005677980771533787 tensor(0.0072)\n",
      "-0.006427461082629938 tensor(0.0060)\n",
      "-0.006090715286565179 tensor(0.0054)\n",
      "-0.006452730544808711 tensor(0.0065)\n",
      "-0.006182041165859438 tensor(0.0045)\n",
      "-0.005382762409615996 tensor(0.0066)\n",
      "-0.005394805205877533 tensor(0.0047)\n",
      "-0.006557318256397223 tensor(0.0043)\n",
      "-0.006414707021117418 tensor(0.0047)\n",
      "-0.006685863338988835 tensor(0.0075)\n",
      "-0.005555825620533185 tensor(0.0088)\n",
      "-0.0061720214527169115 tensor(0.0050)\n",
      "-0.006352807798253827 tensor(0.0077)\n",
      "-0.006003913030611515 tensor(0.0070)\n",
      "-0.0050686068591472375 tensor(0.0058)\n",
      "-0.006480458106760391 tensor(0.0059)\n",
      "-0.006971165127710465 tensor(0.0073)\n",
      "-0.006662276043195285 tensor(0.0068)\n",
      "-0.005360251752677632 tensor(0.0056)\n",
      "-0.006365338396858552 tensor(0.0090)\n",
      "-0.005935364505469681 tensor(0.0069)\n",
      "-0.0062398972473636264 tensor(0.0069)\n",
      "-0.006117604672043543 tensor(0.0194)\n",
      "-0.005709993826804462 tensor(0.0077)\n",
      "-0.005561055621817308 tensor(0.0061)\n",
      "-0.006058690128751699 tensor(0.0146)\n",
      "-0.005782537421443268 tensor(0.0071)\n",
      "-0.006475833358076333 tensor(0.0087)\n",
      "-0.0049967510043197275 tensor(0.0076)\n",
      "-0.0064471760716480935 tensor(0.0073)\n",
      "-0.0069655663473461304 tensor(0.0096)\n",
      "-0.005874383658596398 tensor(0.0062)\n",
      "-0.006497743862138477 tensor(0.0072)\n",
      "-0.005983536417317327 tensor(0.0103)\n",
      "-0.005169570504850716 tensor(0.0063)\n",
      "-0.006527714658843946 tensor(0.0056)\n",
      "-0.006571554065429019 tensor(0.0043)\n",
      "-0.0064780534926558355 tensor(0.0065)\n",
      "-0.0064743595826609905 tensor(0.0089)\n",
      "-0.006383674839843982 tensor(0.0051)\n",
      "-0.007138757710672258 tensor(0.0081)\n",
      "-0.0062609647136021955 tensor(0.0077)\n",
      "-0.006326406671532687 tensor(0.0065)\n",
      "-0.006617381826134844 tensor(0.0053)\n",
      "-0.006301448465556498 tensor(0.0068)\n",
      "-0.005653277796180167 tensor(0.0069)\n",
      "-0.005981751648083393 tensor(0.0050)\n",
      "-0.006511953992581148 tensor(0.0087)\n",
      "-0.006006530852368202 tensor(0.0123)\n",
      "-0.0051177070548470436 tensor(0.0063)\n",
      "-0.0063281631430963815 tensor(0.0102)\n",
      "-0.005635645273424388 tensor(0.0050)\n",
      "-0.005767575035442972 tensor(0.0050)\n",
      "-0.005561296141198836 tensor(0.0072)\n",
      "-0.00678891562173735 tensor(0.0065)\n",
      "-0.005879083413195076 tensor(0.0065)\n",
      "-0.005834684535582408 tensor(0.0106)\n",
      "-0.0069221336026388846 tensor(0.0056)\n",
      "-0.004377423249502629 tensor(0.0087)\n",
      "-0.006674289786432169 tensor(0.0069)\n",
      "-0.006807098882654627 tensor(0.0060)\n",
      "-0.006495935824008482 tensor(0.0070)\n",
      "-0.005776114766155575 tensor(0.0048)\n",
      "-0.006607862718289993 tensor(0.0107)\n",
      "-0.005808636994329206 tensor(0.0067)\n",
      "-0.005954618304327603 tensor(0.0078)\n",
      "-0.006722935006989792 tensor(0.0090)\n",
      "-0.005877022314896748 tensor(0.0064)\n",
      "-0.0070443881030780985 tensor(0.0061)\n"
     ]
    }
   ],
   "source": [
    "mus = []\n",
    "sigma2s = []\n",
    "for i in range(100):\n",
    "    train_res = utils.train_epoch(trainloader, model, NLL, optimizer, cuda=False, regression=True)\n",
    "    swag_model.collect_model(model)\n",
    "#     swag_model.sample(0.0)\n",
    "#     swag_res = utils.eval(testloader, swag_model, NLL, cuda=False, regression=True)\n",
    "#     print(swag_res)\n",
    "    pred, target = utils.predictions(testloader, model, cuda=False, regression=True)\n",
    "    mu = pred[:, 0]\n",
    "    log_sigma2 = pred[:, 1]\n",
    "    sigma2 = np.exp(log_sigma2)\n",
    "#     if train_res['loss'] < 1.04:\n",
    "    mus.append(mu[:, None])\n",
    "    sigma2s.append(sigma2[:, None])\n",
    "    print(train_res['loss'], NLL_(torch.from_numpy(mu), torch.from_numpy(sigma2), torch.from_numpy(target)))\n",
    "mus = np.hstack(mus)\n",
    "sigma2s = np.hstack(sigma2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.571209\n",
      "7.563717\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(target - mus[:, 0]))\n",
    "print(np.linalg.norm(target - np.mean(mus, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0063)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_ens = np.mean(mus, axis=1)\n",
    "sigma2_ens = np.mean(sigma2s, axis=1) + (np.mean(mus**2, axis=1) - mu_ens**2)\n",
    "NLL_(torch.from_numpy(mu_ens), torch.from_numpy(sigma2_ens), torch.from_numpy(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_variances(model):\n",
    "    variances = {}\n",
    "    for module, name in model.params:\n",
    "        mean = module.__getattr__('%s_mean' % name)\n",
    "        sq_mean = module.__getattr__('%s_sq_mean' % name)\n",
    "        var = sq_mean - mean**2\n",
    "        var[var < 1e-6] = 1e-6\n",
    "        sq_mean = var + mean**2\n",
    "        module.__getattr__('%s_sq_mean' % name).copy_(sq_mean)\n",
    "#         variances[name] = (mean, var)\n",
    "#     return variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_variances(swag_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = []\n",
    "sigma2s = []\n",
    "for i in range(30):\n",
    "    swag_model.sample(.1)\n",
    "    pred, target = utils.predictions(testloader, swag_model, cuda=False, regression=True)\n",
    "    mu = pred[:, 0]\n",
    "    log_sigma2 = pred[:, 1]\n",
    "    sigma2 = np.exp(log_sigma2)\n",
    "    mus.append(mu[:, None])\n",
    "    sigma2s.append(sigma2[:, None])\n",
    "mus = np.hstack(mus)\n",
    "sigma2s = np.hstack(sigma2s)\n",
    "# utils.eval(testloader, swag_model, NLL, cuda=False, regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ens = np.mean(mus, axis=1)\n",
    "sigma2_ens = np.mean(sigma2s, axis=1) + (np.mean(mus**2, axis=1) - mu_ens**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0062)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLL_(torch.from_numpy(mu_ens), torch.from_numpy(sigma2_ens), torch.from_numpy(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dropout(m):\n",
    "    if type(m)==torch.nn.modules.dropout.Dropout:\n",
    "        m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionNet.base(num_inputs=x.shape[1], p=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5402362837603581\n",
      "0.15388966759535067\n",
      "0.12901668463236723\n",
      "0.11344834771939595\n",
      "0.09262173675799285\n",
      "0.10322420473485552\n",
      "0.0985243903846587\n",
      "0.07901306988370427\n",
      "0.09098628129420304\n",
      "0.07119136466068779\n",
      "0.08446133922387078\n",
      "0.07745690151638941\n",
      "0.07847753289497324\n",
      "0.09107172715513012\n",
      "0.11626355776580807\n",
      "0.0816406598369802\n",
      "0.07994911788028647\n",
      "0.08548523686223157\n",
      "0.08538187467640748\n",
      "0.08400292147823554\n",
      "0.0884906737601739\n",
      "0.07932468096445197\n",
      "0.07276114854313136\n",
      "0.06950927805187962\n",
      "0.08578159272408578\n",
      "0.0788713868552973\n",
      "0.06854897974751827\n",
      "0.09094947403388327\n",
      "0.09763830840836805\n",
      "0.08604875016594553\n",
      "0.08184230536648156\n",
      "0.07644927836785873\n",
      "0.07272750091994352\n",
      "0.09222482717102266\n",
      "0.08059740594584218\n",
      "0.09518366356425384\n",
      "0.08256989174978116\n",
      "0.07624458828591647\n",
      "0.08213331453366673\n",
      "0.07657771573403394\n",
      "0.07650914548747756\n",
      "0.09012736622194369\n",
      "0.07832227790415211\n",
      "0.07294549218399186\n",
      "0.07575962129119844\n",
      "0.08023765555118631\n",
      "0.07615211614678094\n",
      "0.06782721731807002\n",
      "0.07949292581022692\n",
      "0.09077521350488695\n",
      "0.07219423100675887\n",
      "0.06635707673565766\n",
      "0.08184347711542442\n",
      "0.07741209457272274\n",
      "0.08152934339625566\n",
      "0.0738860662010347\n",
      "0.09035099097458885\n",
      "0.07614036481003357\n",
      "0.10129332021198931\n",
      "0.07193883042512157\n",
      "0.07114212591931762\n",
      "0.08246515963484831\n",
      "0.07450677901627557\n",
      "0.09458881636752496\n",
      "0.07854263494687827\n",
      "0.08728452440774782\n",
      "0.06916531094821152\n",
      "0.08646102365045345\n",
      "0.08934370455405755\n",
      "0.07876096669839326\n",
      "0.0848212960617334\n",
      "0.07710930324711045\n",
      "0.07429077066398916\n",
      "0.09036331048159373\n",
      "0.08446742838239742\n",
      "0.07937561014141303\n",
      "0.07945134791047732\n",
      "0.07846929804624103\n",
      "0.07352381129946502\n",
      "0.08673888488508602\n",
      "0.08134318708390562\n",
      "0.06596722413945068\n",
      "0.0753594317124455\n",
      "0.06686591864802006\n",
      "0.08721854260762163\n",
      "0.07941633409607107\n",
      "0.08309096614321326\n",
      "0.07506039508587807\n",
      "0.08430218120690575\n",
      "0.0735261457166582\n",
      "0.07253484962993266\n",
      "0.06977714275790485\n",
      "0.0668471213793567\n",
      "0.07744336467581944\n",
      "0.07332151987185505\n",
      "0.08524025802398556\n",
      "0.08308035329688566\n",
      "0.09385368658166762\n",
      "0.08529405717220667\n",
      "0.08836605284683603\n",
      "0.07796624357547235\n",
      "0.07755635916016933\n",
      "0.07778642975436784\n",
      "0.07765038087730443\n",
      "0.0678685006468488\n",
      "0.08278541226598005\n",
      "0.07527881040658675\n",
      "0.0745229976767188\n",
      "0.08304772073418791\n",
      "0.07163274369163943\n",
      "0.06206723223779415\n",
      "0.07001124385230813\n",
      "0.07722300585935152\n",
      "0.07551699002460381\n",
      "0.07581529187043083\n",
      "0.06413686333626298\n",
      "0.07398428705535745\n",
      "0.07126712198249693\n",
      "0.07708657540804056\n",
      "0.07645075877858239\n",
      "0.06753637625704643\n",
      "0.08496400413768879\n",
      "0.08209658780035388\n",
      "0.08794209544680258\n",
      "0.08776433226497674\n",
      "0.06906234808443537\n",
      "0.07759728955500578\n",
      "0.08199303726565199\n",
      "0.07639885823487987\n",
      "0.07703499535265014\n",
      "0.08015353089597994\n",
      "0.07272241888419632\n",
      "0.06775875993340345\n",
      "0.07389981851921362\n",
      "0.07470375075097034\n",
      "0.07434283190350294\n",
      "0.06524051649550253\n",
      "0.0738922222581783\n",
      "0.08142692931822722\n",
      "0.06713679337692571\n",
      "0.06958739090403286\n",
      "0.07457308403368045\n",
      "0.07796718516757908\n",
      "0.0720128139934983\n",
      "0.08486014992282472\n",
      "0.08440122407718398\n",
      "0.08225591766088076\n",
      "0.06810428081191718\n",
      "0.08029192985453931\n",
      "0.0740012686373866\n",
      "0.07098026533337812\n",
      "0.06632227113771322\n",
      "0.08244019442215958\n",
      "0.07902110101952119\n",
      "0.08447291190761845\n",
      "0.08908525866895114\n",
      "0.07358504813361315\n",
      "0.07929331805561772\n",
      "0.07580545855072397\n",
      "0.06723803472829022\n",
      "0.09642306455851099\n",
      "0.08283433640180406\n",
      "0.07319834204884913\n",
      "0.07740022328247137\n",
      "0.07207434530598253\n",
      "0.07492063990447485\n",
      "0.07671380949801178\n",
      "0.08352021392443056\n",
      "0.08108263259364011\n",
      "0.07974042814653685\n",
      "0.08560387172681393\n",
      "0.08666887245496908\n",
      "0.0774093828260407\n",
      "0.07518911993665543\n",
      "0.08578151784168415\n",
      "0.07786320580268845\n",
      "0.08458656512850628\n",
      "0.07921300066180545\n",
      "0.06933099846158235\n",
      "0.09796070632477308\n",
      "0.07734152596716874\n",
      "0.0812332979350981\n",
      "0.07390063245555911\n",
      "0.08324965517204365\n",
      "0.06954435691810239\n",
      "0.07124339991531753\n",
      "0.0793340806423711\n",
      "0.07410196065314346\n",
      "0.08411387581909911\n",
      "0.0764492528129989\n",
      "0.07128052708164295\n",
      "0.07951993384907821\n",
      "0.08104280901971338\n",
      "0.07611599907706443\n",
      "0.08013262050019755\n",
      "0.0710644265916923\n",
      "0.07692831044495556\n",
      "0.09200319953737823\n",
      "0.08388325381985547\n",
      "0.07885638968362554\n",
      "0.06874330737317097\n",
      "0.072667243539934\n",
      "0.08308348527922059\n",
      "0.08752561934770794\n",
      "0.07024165668694929\n",
      "0.08342280819615813\n",
      "0.07543532880332013\n",
      "0.07843681459292733\n",
      "0.06833779909112093\n",
      "0.08432397805651837\n",
      "0.08036274190301222\n",
      "0.06899937347742258\n",
      "0.07747996921303131\n",
      "0.06833323982725187\n",
      "0.07812156550644195\n",
      "0.08282594955205613\n",
      "0.07604850022590143\n",
      "0.07875340266056742\n",
      "0.08197366887442452\n",
      "0.0797331835172578\n",
      "0.0758226333892715\n",
      "0.07685820791741002\n",
      "0.08226160927110122\n",
      "0.07240696867616206\n",
      "0.08162114294463231\n",
      "0.08802557165144267\n",
      "0.07851305255116299\n",
      "0.07741175663760558\n",
      "0.07567465929512418\n",
      "0.06967090248833162\n",
      "0.08249132125192286\n",
      "0.07472916608567078\n",
      "0.07152988245201754\n",
      "0.08312444397172727\n",
      "0.08500680670223507\n",
      "0.0787467235002129\n",
      "0.06912552930391748\n",
      "0.07157510154891507\n",
      "0.07815224561468676\n",
      "0.07752093387569316\n",
      "0.0751998198356325\n",
      "0.053433074260606624\n",
      "0.07740843932175384\n",
      "0.07929350532121518\n",
      "0.07017047243131576\n",
      "0.08241946757317364\n",
      "0.07954300992124215\n",
      "0.07233347994236564\n",
      "0.07688663398405496\n",
      "0.07611163861979495\n",
      "0.08399044949617165\n",
      "0.08336137992163012\n",
      "0.07484402642434461\n",
      "0.08376395817958109\n",
      "0.06502421248382759\n",
      "0.06533062296054609\n",
      "0.0777597941753272\n",
      "0.07856787208023422\n",
      "0.06987551621977647\n",
      "0.08182048407038141\n",
      "0.06927447499831287\n",
      "0.07362536670435266\n",
      "0.070277038455051\n",
      "0.08081181008358347\n",
      "0.0767858093408685\n",
      "0.07626535386909607\n",
      "0.06961082562331801\n",
      "0.07427095647002745\n",
      "0.07548217669401057\n",
      "0.07333138028308775\n",
      "0.07320368085058707\n",
      "0.08229661480694639\n",
      "0.06631610168146486\n",
      "0.07420372798902179\n",
      "0.07714404681600798\n",
      "0.09642709977254263\n",
      "0.06954399324393745\n",
      "0.07202936447494925\n",
      "0.08257396791388578\n",
      "0.08434766161360124\n",
      "0.07676909623585183\n",
      "0.07518196974571345\n",
      "0.08562638989489968\n",
      "0.07418336346788754\n",
      "0.07845294563442161\n",
      "0.070719121066193\n",
      "0.08549576013113976\n",
      "0.07511969948289784\n",
      "0.07775725814123531\n",
      "0.0734128844667403\n",
      "0.0768014328320009\n",
      "0.08157269727088699\n",
      "0.07023530336722668\n",
      "0.08223780234292498\n",
      "0.07361031045756292\n",
      "0.06651768354661594\n",
      "0.08103302366544445\n",
      "0.07471948695246741\n",
      "0.0720676578310513\n",
      "0.08277734765665495\n",
      "0.07890668756253572\n",
      "0.07079226454678153\n",
      "0.06688560468092043\n",
      "0.07127186737330779\n",
      "0.07284588796857647\n",
      "0.0684745187903929\n",
      "0.06851960390700422\n",
      "0.07633711536951478\n",
      "0.05601575911813408\n",
      "0.07518696334875359\n",
      "0.06759264276440503\n",
      "0.0815195126800556\n",
      "0.0720660814202326\n",
      "0.05602290090846207\n",
      "0.07663579591353199\n",
      "0.07153647178711549\n",
      "0.07742591724344806\n",
      "0.09319319178101189\n",
      "0.07718049454233564\n",
      "0.07809837542767586\n",
      "0.08758324580233544\n",
      "0.08938082711211537\n",
      "0.08167468310925288\n",
      "0.07794369302578413\n",
      "0.07514403838477834\n",
      "0.07593643475133996\n",
      "0.0913043990116919\n",
      "0.08237576549400119\n",
      "0.07585373151564051\n",
      "0.07017523577758458\n",
      "0.07137458160867427\n",
      "0.0652155040424469\n",
      "0.08749059080773011\n",
      "0.07873799210848424\n",
      "0.07951825695535666\n",
      "0.0763581062823572\n",
      "0.08362258448911286\n",
      "0.08712120164688034\n",
      "0.09130348773412957\n",
      "0.08435038122866657\n",
      "0.07782697157083994\n",
      "0.08267517754988254\n",
      "0.07637640758751854\n",
      "0.08699440205087508\n",
      "0.06477341666729411\n",
      "0.06995595626999396\n",
      "0.08854377343636359\n",
      "0.06820415868713194\n",
      "0.07204174524212725\n",
      "0.08153647645371756\n",
      "0.08170646200638562\n",
      "0.07502734083078794\n",
      "0.07446839735768507\n",
      "0.08875021351095332\n",
      "0.07669603557432107\n",
      "0.07363805475584957\n",
      "0.07147206933486477\n",
      "0.07853665994290981\n",
      "0.07234231440146865\n",
      "0.07340006483042784\n",
      "0.08287392147408865\n",
      "0.07528744587263099\n",
      "0.07822065786498028\n",
      "0.0823620515146013\n",
      "0.06885612708858296\n",
      "0.08439716376254346\n",
      "0.08070911968222286\n",
      "0.08265989383239389\n",
      "0.07962502466650766\n",
      "0.07594553961867088\n",
      "0.07034748175386187\n",
      "0.08925250521133675\n",
      "0.073275186551488\n",
      "0.07649453453466101\n",
      "0.08632573993097212\n",
      "0.0806933488736182\n",
      "0.08414996011569212\n",
      "0.07878118906753734\n",
      "0.09543168533002025\n",
      "0.0861577685944448\n",
      "0.07712868571752124\n",
      "0.07297529870452345\n",
      "0.08602321403943182\n",
      "0.08220053754808346\n",
      "0.08360467297581482\n",
      "0.0696979695400036\n",
      "0.06570154885328712\n",
      "0.08626761265387173\n",
      "0.06579663770364394\n",
      "0.06654824931341076\n",
      "0.07937493683250506\n",
      "0.06064612906148882\n",
      "0.06350148846886879\n",
      "0.09168311022892603\n",
      "0.08323133430597177\n",
      "0.07362113865863541\n",
      "0.07129236694800425\n",
      "0.07390035675249577\n",
      "0.09669157988901768\n",
      "0.06261551667244063\n",
      "0.07011156912575947\n",
      "0.06192933120748028\n",
      "0.05494882420614758\n",
      "0.06587540175825887\n",
      "0.050857584954221796\n",
      "0.06635018037581275\n",
      "0.07053604149926403\n",
      "0.050760674990662544\n",
      "0.07239177574915914\n",
      "0.06535962786369312\n",
      "0.062090695371556925\n",
      "0.05673206944787368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06159819929237884\n",
      "0.05819298049573242\n",
      "0.05988691911757193\n",
      "0.06978463978803942\n",
      "0.0540695065835145\n",
      "0.07026402157822313\n",
      "0.04688631634691592\n",
      "0.05521836794844461\n",
      "0.07288472165789785\n",
      "0.06378724114458233\n",
      "0.04587315088357539\n",
      "0.06250187709465521\n",
      "0.06329998595140604\n",
      "0.06756369685644943\n",
      "0.06231305645823119\n",
      "0.05152142679161096\n",
      "0.07594056885969402\n",
      "0.0657826301638734\n",
      "0.061531113712661095\n",
      "0.06425784732884694\n",
      "0.05403401350403286\n",
      "0.06183905511724175\n",
      "0.062355864270889946\n",
      "0.05952080459548211\n",
      "0.0627036313623131\n",
      "0.06442611854107673\n",
      "0.0527094538626307\n",
      "0.05886055375537506\n",
      "0.06300258051918547\n",
      "0.05677545466625224\n",
      "0.0594008770945346\n",
      "0.05560034309969091\n",
      "0.06443465938967925\n",
      "0.060893544875541335\n",
      "0.06104174792427397\n",
      "0.05977396664061816\n",
      "0.05290725504655675\n",
      "0.057350069937695196\n",
      "0.05781542624871139\n",
      "0.04857472861329516\n",
      "0.07386178357781689\n",
      "0.05512175062069313\n",
      "0.061851439264479335\n",
      "0.05450417392969049\n",
      "0.06843971623982333\n",
      "0.045800444632924375\n",
      "0.05399316392173586\n",
      "0.06712663811011232\n",
      "0.05296586588545967\n",
      "0.05622456193787062\n",
      "0.0577208815154842\n",
      "0.05878001340828129\n",
      "0.06429505978100199\n",
      "0.07005481773407918\n",
      "0.061277165979531124\n",
      "0.0647499413025004\n",
      "0.06175840835083432\n",
      "0.06333426089600229\n",
      "0.056278978632020006\n",
      "0.06216139063341196\n",
      "0.0648699334853203\n",
      "0.06813924175502946\n",
      "0.0587193497713262\n",
      "0.06206321694158453\n",
      "0.05298959633335255\n",
      "0.0721136001660648\n",
      "0.06743693958451809\n",
      "0.05055132373646568\n",
      "0.05319013106703218\n",
      "0.062401362141587154\n",
      "0.06146414241685489\n",
      "0.0503586774821509\n",
      "0.07219436177046176\n",
      "0.04960433328001843\n",
      "0.06132714793859576\n",
      "0.061949549105064665\n",
      "0.05904846071449393\n",
      "0.04605069763775888\n",
      "0.06287645226799365\n",
      "0.058262153887164506\n",
      "0.05519616574952307\n",
      "0.06096465579171226\n",
      "0.06692432574023631\n",
      "0.07297001482648255\n",
      "0.05006153685486479\n",
      "0.059186575893408304\n",
      "0.07277792886517324\n",
      "0.061791086630211545\n",
      "0.06745830594614233\n",
      "0.0708761551009558\n",
      "0.06128984285645501\n",
      "0.06774537876333153\n",
      "0.06229185317355835\n",
      "0.05484267728189713\n",
      "0.06553368058240368\n",
      "0.05065146230845595\n",
      "0.049421615022986407\n",
      "0.06259143904704635\n",
      "0.06789463905386359\n",
      "0.06207034676391452\n",
      "0.05876807418555945\n",
      "0.05189988229239033\n",
      "0.05093234396571967\n",
      "0.052103583875841526\n",
      "0.0711371041717547\n",
      "0.06331069536802177\n",
      "0.06386072218120478\n",
      "0.055563027764457595\n",
      "0.06110404957009707\n",
      "0.058880968099617265\n",
      "0.05303494303452956\n",
      "0.06316809754742105\n",
      "0.04913516610712869\n",
      "0.05792264815628868\n",
      "0.05455595830455639\n",
      "0.06076831023890498\n",
      "0.059175249605751414\n",
      "0.07120891615372892\n",
      "0.054616707200518665\n",
      "0.05279486110617151\n",
      "0.06241628016457576\n",
      "0.05118499656280648\n",
      "0.07655011809477198\n",
      "0.052774247376916855\n",
      "0.05874376695145853\n",
      "0.06119876983745867\n",
      "0.052673328124941084\n",
      "0.061094965845875975\n",
      "0.06261147550555862\n",
      "0.058637796183341806\n",
      "0.06080247097875914\n",
      "0.0530338684887978\n",
      "0.07223860524060931\n",
      "0.05482840245963181\n",
      "0.0663470135892037\n",
      "0.04938361530117894\n",
      "0.05016882937207287\n",
      "0.05226859438573009\n",
      "0.06108834503724481\n",
      "0.04795693183382024\n",
      "0.05694985982225742\n",
      "0.05785928341088402\n",
      "0.05605448860823918\n",
      "0.06074100572214823\n",
      "0.06662375648571853\n",
      "0.04775771987367598\n",
      "0.06001017354891672\n",
      "0.06926254198714812\n",
      "0.04729353305548966\n",
      "0.05981792522534415\n",
      "0.05959849786253469\n",
      "0.056625411357423897\n",
      "0.07197830822669647\n",
      "0.051213782733636595\n",
      "0.0666836327362138\n",
      "0.0634706253018228\n",
      "0.0582994605648076\n",
      "0.058623642411163884\n",
      "0.06639610531507033\n",
      "0.05909594009526445\n",
      "0.05843253386116183\n",
      "0.06619549621661883\n",
      "0.06295358418947644\n",
      "0.06795702740872776\n",
      "0.06777919623483131\n",
      "0.06272946373571324\n",
      "0.06661253928058364\n",
      "0.05895327576800044\n",
      "0.06408019236689895\n",
      "0.05257745240207694\n",
      "0.066443081808473\n",
      "0.05528375545488273\n",
      "0.053967081804631854\n",
      "0.05621334883051181\n",
      "0.04941765374802862\n",
      "0.06672905041301809\n",
      "0.05905344933288713\n",
      "0.05527803954894279\n",
      "0.04654476251797094\n",
      "0.05596921506513357\n",
      "0.054132288139936\n",
      "0.05487675237511664\n",
      "0.06587121204353351\n",
      "0.08137551190760088\n",
      "0.0626734180561744\n",
      "0.06161904702748921\n",
      "0.0637320334617632\n",
      "0.06063599645011642\n"
     ]
    }
   ],
   "source": [
    "for i in range(600):\n",
    "    train_res = utils.train_epoch(trainloader, model, NLL, optimizer, cuda=False, regression=True)\n",
    "    print(train_res['loss'])\n",
    "    if i == 400:\n",
    "        utils.adjust_learning_rate(optimizer, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "train_dropout(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = []\n",
    "sigma2s = []\n",
    "for i in range(30):\n",
    "    pred, target = utils.predictions(testloader, model, cuda=False, regression=True)\n",
    "    mu = pred[:, 0]\n",
    "    log_sigma2 = pred[:, 1]\n",
    "    sigma2 = np.exp(log_sigma2)\n",
    "    mus.append(mu[:, None])\n",
    "    sigma2s.append(sigma2[:, None])\n",
    "mus = np.hstack(mus)\n",
    "sigma2s = np.hstack(sigma2s)\n",
    "# utils.eval(testloader, swag_model, NLL, cuda=False, regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ens = np.mean(mus, axis=1)\n",
    "sigma2_ens = np.mean(sigma2s, axis=1) + (np.mean(mus**2, axis=1) - mu_ens**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0223)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLL_(torch.from_numpy(mu_ens), torch.from_numpy(sigma2_ens), torch.from_numpy(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
